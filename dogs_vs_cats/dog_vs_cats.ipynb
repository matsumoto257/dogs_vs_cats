{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験的にコードを実行したい場合はこのファイルで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "from torch import utils\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional\n",
    "from torchvision import transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './dogs-vs-cats-redux-kernels-edition'\n",
    "train_path = os.path.join(data_dir, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps is available\n"
     ]
    }
   ],
   "source": [
    "# mpsを利用できるか確認\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print('mps is available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", required=True)\n",
    "    parser.add_argument(\"--device\", default=\"mps\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data_dir = pathlib.Path(args.data_dir)\n",
    "\n",
    "    print(data_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_center_crop_transform():\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(dataset):\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        return get_labels(dataset.dataset)[dataset.indices]\n",
    "    else:\n",
    "        return np.array([img[1] for img in dataset.imgs])\n",
    "    \n",
    "\n",
    "def setup_train_val_datasets(data_dir, dryrun=False):\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, \"train\"),\n",
    "        transform=setup_center_crop_transform(),\n",
    "    )\n",
    "    labels = get_labels(dataset)\n",
    "    return labels\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, \"train\"),\n",
    "        transform=setup_center_crop_transform(),\n",
    "    )\n",
    "\n",
    "if isinstance(dataset, torch.utils.data.Subset):\n",
    "    get_labels(dataset.dataset)[dataset.indices]\n",
    "else:\n",
    "    np.array([img[1] for img in dataset.imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array([img[1] for img in dataset.imgs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0, 'dog': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, \"train\"),\n",
    "        transform=setup_center_crop_transform()\n",
    "    ).class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in dataset.imgs:\n",
    "    print(np.array(img[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list_iterator'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [1, 2, 3, 4, 5]\n",
    "#listオブジェクトをイテレータオブジェクトに\n",
    "b_iter = iter(b)\n",
    "print(type(b_iter))\n",
    "next(b_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "#listオブジェクトをイテレータオブジェクトに\n",
    "iter_a = iter(a)\n",
    "#nextメソッドで一つづつ取り出す\n",
    "print(next(iter_a))\n",
    "print(next(iter_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "marks = [65, 71, 68, 74, 61]\n",
    "\n",
    "# convert list to iterator\n",
    "iterator_marks = iter(marks)\n",
    "\n",
    "# the next element is the first element\n",
    "marks_1 = next(iterator_marks)\n",
    "print(marks_1)\n",
    "\n",
    "# find the next element which is the second element\n",
    "marks_2 = next(iterator_marks)\n",
    "print(marks_2)\n",
    "\n",
    "# Output: 65\n",
    "#         71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "みかん\n",
      "りんご\n",
      "バナナ\n",
      "パイナップル\n"
     ]
    }
   ],
   "source": [
    "l = [\"みかん\", \"りんご\", \"バナナ\", \"パイナップル\"]\n",
    "it = iter(l)\n",
    "while True:\n",
    "    try:\n",
    "        print(next(it))\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 24997 24998 24999]\n",
      "[0 0 0 ... 1 1 1]\n",
      "20000\n",
      "5000\n",
      "(array([15792, 17742,  8931, ..., 17930,  3981, 19080]), array([14589,  4039, 13467, ...,  9871, 21646, 12706]))\n",
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "def setup_center_crop_transform():\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def get_labels(dataset):\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        return get_labels(dataset.dataset)[dataset.indices]\n",
    "    else:\n",
    "        return np.array([img[1] for img in dataset.imgs])\n",
    "    \n",
    "\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    os.path.join(data_dir, \"train\"),\n",
    "    transform=setup_center_crop_transform(),\n",
    ")\n",
    "labels = get_labels(dataset)\n",
    "\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "y = np.array(labels)\n",
    "#sklearn.model_selection.StratifiedShuffleSplit\n",
    "#データセット全体のクラスの偏りを保持しながら、データセットを分割.データの重複が許容されていると分かる.必ずしも全てのデータがvalidationのデータセットに一度使われるわけではない.\n",
    "#検証用データが重複したり，ある学習用データが学習されなかったりするので，あまり使われないイメージ\n",
    "splitter = sklearn.model_selection.StratifiedShuffleSplit(\n",
    "    n_splits=1, train_size=0.8, random_state=0    #n_splits:分割&シャッフル回数\n",
    ")\n",
    "train_indices, val_indices = next(splitter.split(x, y))\n",
    "print(x)\n",
    "print(y)\n",
    "print(len(train_indices))\n",
    "print(len(val_indices))\n",
    "print(next(splitter.split(x, y)))\n",
    "print(type(splitter.split(x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57d4740e4fb0ef2b6047ff3b46de79aa600a3fecbf2b4d3806cb9dbdc75e49d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
