{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験的にコードを実行したい場合はこのファイルで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "from torch import utils\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional\n",
    "from torchvision import transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './dogs-vs-cats-redux-kernels-edition'\n",
    "train_path = os.path.join(data_dir, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps is available\n"
     ]
    }
   ],
   "source": [
    "# mpsを利用できるか確認\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print('mps is available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", required=True)\n",
    "    parser.add_argument(\"--device\", default=\"mps\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data_dir = pathlib.Path(args.data_dir)\n",
    "\n",
    "    print(data_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_center_crop_transform():\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(dataset):\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        return get_labels(dataset.dataset)[dataset.indices]\n",
    "    else:\n",
    "        return np.array([img[1] for img in dataset.imgs])\n",
    "    \n",
    "\n",
    "def setup_train_val_datasets(data_dir, dryrun=False):\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, \"train\"),\n",
    "        transform=setup_center_crop_transform(),\n",
    "    )\n",
    "    labels = get_labels(dataset)\n",
    "    return labels\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, \"train\"),\n",
    "        transform=setup_center_crop_transform(),\n",
    "    )\n",
    "\n",
    "if isinstance(dataset, torch.utils.data.Subset):\n",
    "    get_labels(dataset.dataset)[dataset.indices]\n",
    "else:\n",
    "    np.array([img[1] for img in dataset.imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array([img[1] for img in dataset.imgs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0, 'dog': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, \"train\"),\n",
    "        transform=setup_center_crop_transform()\n",
    "    ).class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in dataset.imgs:\n",
    "    print(np.array(img[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
