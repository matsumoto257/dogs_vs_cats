{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験的にコードを実行したい場合はこのファイルで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pathlib\n",
    "import copy\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "from torch import utils\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional\n",
    "from torchvision import transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './dogs-vs-cats-redux-kernels-edition'\n",
    "train_path = os.path.join(data_dir, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps is available\n"
     ]
    }
   ],
   "source": [
    "# mpsを利用できるか確認\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print('mps is available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", required=True)\n",
    "    parser.add_argument(\"--device\", default=\"mps\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data_dir = pathlib.Path(args.data_dir)\n",
    "\n",
    "    print(data_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_center_crop_transform():\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(dataset):\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        return get_labels(dataset.dataset)[dataset.indices]\n",
    "    else:\n",
    "        return np.array([img[1] for img in dataset.imgs])\n",
    "    \n",
    "\n",
    "def setup_train_val_datasets(data_dir, dryrun=False):\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, \"train\"),\n",
    "        transform=setup_center_crop_transform(),\n",
    "    )\n",
    "    labels = get_labels(dataset)\n",
    "    return labels\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, \"train\"),\n",
    "        transform=setup_center_crop_transform(),\n",
    "    )\n",
    "\n",
    "if isinstance(dataset, torch.utils.data.Subset):\n",
    "    get_labels(dataset.dataset)[dataset.indices]\n",
    "else:\n",
    "    np.array([img[1] for img in dataset.imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['./dogs-vs-cats-redux-kernels-edition/test/unknown/1.jpg',\n",
       "       './dogs-vs-cats-redux-kernels-edition/test/unknown/10.jpg',\n",
       "       './dogs-vs-cats-redux-kernels-edition/test/unknown/100.jpg', ...,\n",
       "       './dogs-vs-cats-redux-kernels-edition/test/unknown/9997.jpg',\n",
       "       './dogs-vs-cats-redux-kernels-edition/test/unknown/9998.jpg',\n",
       "       './dogs-vs-cats-redux-kernels-edition/test/unknown/9999.jpg'],\n",
       "      dtype='<U59')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([img[0] for img in dataset.imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0, 'dog': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, \"train\"),\n",
    "        transform=setup_center_crop_transform()\n",
    "    ).class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in dataset.imgs:\n",
    "    print(np.array(img[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list_iterator'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [1, 2, 3, 4, 5]\n",
    "#listオブジェクトをイテレータオブジェクトに\n",
    "b_iter = iter(b)\n",
    "print(type(b_iter))\n",
    "next(b_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "#listオブジェクトをイテレータオブジェクトに\n",
    "iter_a = iter(a)\n",
    "#nextメソッドで一つづつ取り出す\n",
    "print(next(iter_a))\n",
    "print(next(iter_a))\n",
    "print(next(iter_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "71\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "marks = [65, 71, 68, 74, 61]\n",
    "\n",
    "# convert list to iterator\n",
    "iterator_marks = iter(marks)\n",
    "\n",
    "# the next element is the first element\n",
    "marks_1 = next(iterator_marks)\n",
    "print(marks_1)\n",
    "\n",
    "# find the next element which is the second element\n",
    "marks_2 = next(iterator_marks)\n",
    "print(marks_2)\n",
    "\n",
    "print(next(iterator_marks))\n",
    "\n",
    "# Output: 65\n",
    "#         71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "みかん\n",
      "りんご\n",
      "バナナ\n",
      "パイナップル\n"
     ]
    }
   ],
   "source": [
    "l = [\"みかん\", \"りんご\", \"バナナ\", \"パイナップル\"]\n",
    "it = iter(l)\n",
    "while True:\n",
    "    try:\n",
    "        print(next(it))\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 25000\n",
      "    Root location: ./dogs-vs-cats-redux-kernels-edition/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
      "               CenterCrop(size=(224, 224))\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "Compose(\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear), antialias=None)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "<torch.utils.data.dataset.Subset object at 0x16dcb36d0>\n"
     ]
    }
   ],
   "source": [
    "# trainデータをtrain_val_splitし、trainのインデックスとvalのインデックをそれぞれ取得\n",
    "def setup_train_val_split(labels, dryrun=False, seed=0):\n",
    "    x = np.arange(len(labels))\n",
    "    y = np.array(labels)\n",
    "    #sklearn.model_selection.StratifiedShuffleSplit\n",
    "    #データセット全体のクラスの偏りを保持しながら、データセットを分割.データの重複が許容されていると分かる.必ずしも全てのデータがvalidationのデータセットに一度使われるわけではない.\n",
    "    #検証用データが重複したり，ある学習用データが学習されなかったりするので，あまり使われないイメージ\n",
    "    #インスタンスの作成\n",
    "    splitter = sklearn.model_selection.StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        train_size=0.8, \n",
    "        random_state=seed    #n_splits:分割&シャッフル回数\n",
    "    )\n",
    "    #上で分割したtrainデータのインデックス=train_indices,valデータのインデックス=val_indices\n",
    "    #x:分割するデータのインデックスの配列,y:それに対応するラベルの配列\n",
    "    #next()でsplitter.split(x, y)から要素を取り出す\n",
    "    train_indices, val_indices = next(splitter.split(x, y))\n",
    "    #dryrun=True→ランダムに10個run\n",
    "    if dryrun:\n",
    "        train_indices = np.random.choice(train_indices, 10, replace=False)  #train_indicesを更新\n",
    "        val_indices = np.random.choice(val_indices, 10, replace=False)\n",
    "\n",
    "    return train_indices, val_indices\n",
    "\n",
    "# transform\n",
    "def setup_center_crop_transform():\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "#データ拡張\n",
    "def setup_crop_flip_transform():\n",
    "    \"\"\"\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    指定した平均, 標準偏差でTensorを正規化\n",
    "    平均、標準偏差はresnet50のデフォルトの値を使用\n",
    "    https://discuss.pytorch.org/t/what-is-the-correct-pytorch-resnet50-input-normalization-intensity-range/147540\n",
    "    \"\"\"\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomResizedCrop(224),    # random crop\n",
    "            transforms.RandomHorizontalFlip(),    # random flip:画像を左右反転\n",
    "            transforms.ToTensor(),  # 画像データをtensor形式に\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])   \n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# trainデータの正解ラベル(dog=1,cat=0)を返す\n",
    "def get_labels(dataset):\n",
    "    # 結局このif文は何がしたいのか分からない、このif文はfalseになる\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        return get_labels(dataset.dataset)[dataset.indices]\n",
    "    else:\n",
    "        #traiデータの正解ラベルを全て１次元配列で出力\n",
    "        return np.array([img[1] for img in dataset.imgs])  # torchvision.datasets.ImageFolder.imgs : List of (image path, class_index) tuples\n",
    "\n",
    "\n",
    "#setup_train_val_splitで分割したデータをdatasetに変換\n",
    "def setup_train_val_datasets(data_dir, dryrun=False):\n",
    "    #torchvision.datasets.ImageFolderを使用してデータセットの作成\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, \"train\"),   #trainデータの入っているディレクトリのパス\n",
    "        transform=setup_center_crop_transform()   #transformを指定することによって画像に対する前処理をすることができる\n",
    "    )\n",
    "    # labels=正解ラベルの配列\n",
    "    labels = get_labels(dataset)\n",
    "    # setup_train_val_split\n",
    "    train_indices, val_indices = setup_train_val_split(labels, dryrun)\n",
    "\n",
    "    #訓練データセットと検証データセットにデータセットを分割する際はSubsetクラスを用いる\n",
    "    #元のデータセットから指定したインデックスだけ使用するデータセットを作成できる.学習及びテストに使用するインデックスを予め作成しておくことで、データセットを分割できる.\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = setup_train_val_datasets(\n",
    "        data_dir\n",
    "    )\n",
    "\n",
    "train_dataset = copy.deepcopy(\n",
    "        train_dataset\n",
    "    )  # transformを設定した際にval_datasetに影響したくない\n",
    "\n",
    "print(train_dataset.dataset)\n",
    "\n",
    "# train_dataset.dataset.transfrom = setup_crop_flip_transform()\n",
    "train_dataset.dataset.transform = setup_crop_flip_transform()\n",
    "\n",
    "print(train_dataset.dataset.transform)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5834"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_center_crop_transform():\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, \"test\")   #trainデータの入っているディレクトリのパス\n",
    "        , transform=setup_center_crop_transform()\n",
    "    )\n",
    "image_ids = [\n",
    "    os.path.splitext(os.path.basename(path))[0] for path, _ in dataset.imgs   # torchvision.datasets.ImageFolder.imgs : List of (image path, class_index) tuples\n",
    "]\n",
    "dataset.imgs\n",
    "image_ids.index('3')\n",
    "dataset.imgs[4723]\n",
    "\n",
    "def my_index(l, x, default=False):\n",
    "    if x in l:\n",
    "        return l.index(x)\n",
    "    else:\n",
    "        return default\n",
    "    \n",
    "my_index(image_ids, '4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([os.path.splitext(os.path.basename(path)) for path, _ in dataset.imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = torch.utils.data.Subset(dataset, range(0, 100))  #\n",
    "image_ids = image_ids[:100]\n",
    "len(image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x283779a60>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/matsumoto/Documents/work/kaggle/dogs_vs_cats')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlib.Path().cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x122e1cc10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "model = nn.Linear(1, 1)\n",
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> --> configは辞書型\n",
      "{'optimizer': {'SGD': {'name': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'weight_decay': 0.0001}, 'Adam': {'name': 'Adam', 'lr': 0.05}}, 'scheduler': {'cosineannealing': {'name': 'CosineAnnealingLR'}}, 'architecture': {'resnet50': {'name': 'resnet50', 'weights': 'IMAGENET1K_V2'}}, 'test': [{'a': None, 'name': 'a'}, {'b': None, 'name': 'b'}, {'c': None, 'name': 'c'}]}\n",
      "{'SGD': {'name': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'weight_decay': 0.0001}, 'Adam': {'name': 'Adam', 'lr': 0.05}}\n",
      "{'name': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'weight_decay': 0.0001}\n",
      "Linear(in_features=1, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "#yamlを読み込んで**kwargs（キーワード引数）で取り込む\n",
    "import yaml\n",
    "path = './config.yaml'\n",
    "with open(path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "print(type(config), '--> configは辞書型')\n",
    "#**kwargs : 辞書型のキーワード引数\n",
    "#func(**辞書型オブジェクトの名前)\n",
    "def func(**kwargs):\n",
    "    print(kwargs)\n",
    "    #これはダメ\n",
    "    #print(**kwargs)\n",
    "\n",
    "func(**config)\n",
    "func(**config['optimizer'])\n",
    "func(**config['optimizer']['SGD'])  #func(name='SGD', lr=0.05, momentum=0.9)と同じ\n",
    "\n",
    "#モデルば適当にtorch.nn.Linear(1, 1)\n",
    "model = torch.nn.Linear(1, 1)\n",
    "def make_optimizer(params, name, **kwargs):\n",
    "    return torch.optim.__dict__[name](params, **kwargs)\n",
    "params = model.parameters()\n",
    "make_optimizer(params, **config['optimizer']['SGD'])\n",
    "# 上記はmake_optimizer(params, name='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)と同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'SGD', 'lr': 0.01, 'momentum': 0.9}\n",
      "{'SGD': {'name': 'SGD', 'lr': 0.01, 'momentum': 0.9}, 'Adam': {'name': 'Adam'}}\n",
      "SGD\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "path = './config.yaml'\n",
    "with open(path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "def func(**kwargs):\n",
    "    print(kwargs)\n",
    "\n",
    "i = config['optimizer_v2']['target']\n",
    "func(**config['optimizer_v2']['ourputs'][i])\n",
    "func(**config['optimizer_v3'])\n",
    "\n",
    "#モデルば適当にtorch.nn.Linear(1, 1)\n",
    "model = torch.nn.Linear(1, 1)\n",
    "def make_optimizer(params, name, **kwargs):\n",
    "    return torch.optim.__dict__[name](params, **kwargs)\n",
    "params = model.parameters()\n",
    "# make_optimizer(params, **config['optimizer_v2'])\n",
    "\n",
    "if 'SGD' in config['optimizer_v3']:\n",
    "    print('SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = (1, 2, 3, 4, 1, 2)\n",
    "type(t)\n",
    "t[0]\n",
    "t.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sub(x,y):\n",
    "    print(x-y)\n",
    "\n",
    "def sum(a,b,x,y):\n",
    "    sub(x,y)\n",
    "    return a + b\n",
    "\n",
    "sum(a=1,b=2,x=3,y=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "hello world\n",
      "hello world\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "#ただepoch回実行したい場合\n",
    "print(list(range(3)))\n",
    "for epoch in range(3):\n",
    "    print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'resnet50', 'weights': 'torchvision.models.ResNet50_Weights.IMAGENET1K_V2'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.optim.lr_scheduler.CosineAnnealingLR"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "path = './config.yaml'\n",
    "with open(path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "def func(**kwargs):\n",
    "    print(kwargs)\n",
    "\n",
    "\n",
    "# torch.optim.__dict__[name](params, **kwargs)\n",
    "\n",
    "func(**config['architecture']['resnet50'])\n",
    "\n",
    "target_scheduler='CosineAnnealingLR'\n",
    "torch.optim.lr_scheduler.__dict__[target_scheduler]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57d4740e4fb0ef2b6047ff3b46de79aa600a3fecbf2b4d3806cb9dbdc75e49d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
